---
title: "Eco 5316 Time Series Econometrics"
subtitle: Lecture 2 Autoregressive (AR) processes
output:
  beamer_presentation:
    includes:
        in_header: lecturesfmt.tex 
    # keep_tex: yes
    highlight: tango
    fonttheme: professionalfonts
    # incremental: true
fontsize: 9pt
urlcolor: magenta
linkcolor: magenta
editor_options: 
  chunk_output_type: console
---

## Outline

1. Features of Time Series
2. Box-Jenkins methodology
3. Autoregressive Model AR($p$)
4. Autocorrelation Function (ACF) 
5. Partial Autocorrelation Function (PACF)
6. Portmanteau Test - Box-Pierce test and Ljung-Box test
7. Information Criteria - Akaike (AIC) and Schwarz-Bayesian (BIC)
8. Example: AR model for Real GNP growth rate



## Trend, Seasonality, Structural Change, Volatility, Outliers

- **trend** is a tendency of the time series to either grow or decline over the long term 

- **seasonality** refers to regular patterns arising in economic activity due to calendar (on quarterly, monthly, day of week basis)

- **cycles** refer to patterns where the data rises and falls that are not of fixed period/duration (so while seasonal pattern has constant length cyclic pattern
has variable length)

\medskip

- timing of peaks and troughs is predictable with seasonal data, but unpredictable in the long term with cyclic data



```{r set-options, echo=FALSE, cache=FALSE}
options(width = 100)
```

```{r echo=FALSE}
knitr::knit_hooks$set(mysize = function(before, options, envir) {
  if (before) 
    return(options$size)
})
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(message=FALSE, warning=FALSE,  mysize=TRUE, size='\\scriptsize')
```


```{r, echo=FALSE}
library(magrittr)
library(tidyverse)
library(tidyquant)
library(timetk)
library(tis)
# library(lubridate)
# library(xts)
library(scales)
library(egg)
library(grid)

library(Quandl)
Quandl.api_key('DLk9RQrfTVkD4UTKc7op')
```

```{r, echo=FALSE}
rec_dates <- nberDates() %>%
        as_tibble() %>%
        mutate_all(. %>% as.character() %>% as.Date(format = "%Y%m%d"))

# plot with recession shading
g <-ggplot() +
      # geom_hline(yintercept = 0, color = "gray60") +
      geom_rect(data = rec_dates, aes(xmin = Start, xmax = End, ymin = -Inf, ymax = Inf), alpha = 0.1) +
      geom_line(aes(x = date, y = value), size=0.5, color="blue") +
      labs(x = "", y = "") +
      theme_bw() +
      theme(plot.title = element_text(size = 15), 
            legend.position = "none")
```



## Trend, Seasonality, Structural Change, Volatility, Outliers

```{r, echo=FALSE, fig.height=4, fig.width=11}
# Retail and Food Services Sales
x_tbl <- "RSAFSNA" %>%
    tq_get(get = "economic.data", from = "1992-01-01", to = "2018-12-31") %>%
    mutate(sales = as.numeric(price),
           sales_dlog = log(sales) - lag(log(sales)),
           sales_pch_mom = price/lag(sales) - 1) %>%
    select(date, c(sales, sales_dlog, sales_pch_mom)) %>%
    gather(measure, value, c(sales, sales_dlog, sales_pch_mom)) 

g %+% {x_tbl %>% filter(measure == "sales")} +
    scale_y_continuous(limits = c(100000, 600000), breaks = seq(100000, 600000, 100000), labels = number_format(accuracy = 1)) +
    xlim(as.Date(c("1992-02-01", "2018-12-31"))) +
    labs(title = "Retail and Food Services Sales, Millions of Dollars")

g %+% {x_tbl %>% filter(measure == "sales_pch_mom")} +
    # geom_hline(yintercept = 0, color = "gray60")+
    scale_y_continuous(limits = c(-0.3, 0.2), breaks = seq(-0.3, 0.2, 0.1), labels = percent_format(accuracy = 1)) +
    xlim(as.Date(c("1992-02-01", "2018-12-31"))) +
    labs(title = "Retail and Food Services Sales, Percentage Change")
```

$_{}$ \hspace*{0.75cm} https://fred.stlouisfed.org/graph/?g=mHDh



## Trend, Seasonality, Structural Change, Volatility, Outliers

```{r, echo=FALSE, fig.height=4, fig.width=11}
# Retail and Food Services Sales
x_tbl <- c("RSAFS", "RSAFSNA") %>%
    tq_get(get = "economic.data", from = "1992-01-01", to = "2018-12-31") %>%
    mutate(symbol_label = recode(symbol, RSAFS = "SA", RSAFSNA = "NSA")) %>%
    group_by(symbol) %>%
    mutate(sales = as.numeric(price),
           sales_dlog = log(sales) - lag(log(sales)),
           sales_pch_mom = sales/lag(sales) - 1,
           err = sales_dlog - sales_pch_mom) %>%
    ungroup() %>%
    select(symbol, symbol_label, date, c(sales, sales_dlog, sales_pch_mom, err)) %>%
    gather(measure, value, c(sales, sales_dlog, sales_pch_mom, err)) 

g2 <- ggplot() +
        geom_rect(data = rec_dates, aes(xmin = Start, xmax = End, ymin = -Inf, ymax = Inf), alpha = 0.15) +
        geom_line(aes(x = date, y = value, colour = symbol_label, size = symbol_label)) +
        scale_color_manual(values = c("blue", "black")) +
        scale_size_manual(values = c(0.5, 1)) +
        xlim(as.Date(c("1992-02-01", "2018-12-31"))) +
        labs(subtitle = "Not Seasonally Adjusted (SA) vs Seasonally Adjusted (SA)",
             x = "", y = "", color = "", size = "") +
        theme_bw() +
        theme(plot.title = element_text(size = 15), 
              legend.position = c(0.95, 0.935),
              legend.key =  element_blank(),
              legend.background = element_blank())

g2 %+% {x_tbl %>% filter(measure == "sales")} +
    scale_y_continuous(limits = c(100000, 600000), breaks = seq(100000, 600000, 100000), labels = number_format(accuracy = 1)) +
    labs(title = "Retail and Food Services Sales, Millions of Dollars")

g2 %+% {x_tbl %>% filter(measure == "sales_pch_mom")} +
    # geom_hline(yintercept = 0, color = "gray60")+
    scale_y_continuous(limits = c(-0.3, 0.2), breaks = seq(-0.3, 0.2, 0.1), labels = percent_format(accuracy = 1)) +
    labs(title = "Retail and Food Services Sales, Percentage Change")
```

$_{}$ \hspace*{0.75cm} https://fred.stlouisfed.org/graph/?g=mHDh



## Trend, Seasonality, Structural Change, Volatility, Outliers

```{r, echo=FALSE, fig.height=4, fig.width=11}
# Real GDP
x_tbl <- "GDPC1" %>%
    tq_get(get = "economic.data", from = "1947-01-01", to = "2018-12-31") %>%
    rename(rgdp = price) %>%
    mutate(rgdp_dlog = log(rgdp) - lag(log(rgdp)),
           rgdp_pch_qoq = rgdp/lag(rgdp) - 1,
           err = rgdp_dlog - rgdp_pch_qoq) %>%
    gather(measure, value, c(rgdp, rgdp_dlog, rgdp_pch_qoq, err)) 

g %+% {x_tbl %>% filter(measure == "rgdp")} +
    scale_y_continuous(limits = c(0, 20000), breaks = seq(5000, 20000, 5000), labels = number_format(accuracy = 1)) +
    xlim(as.Date(c("1947-01-01", "2018-12-31"))) +
    labs(title = "U.S. Real GDP, Billion of 2012 Dollars, Seasonally Adjusted Annual Rate")

g %+% {x_tbl %>% filter(measure == "rgdp_dlog")} +
    # geom_hline(yintercept = 0, color = "gray60")+
    # scale_y_continuous(limits = c(-0.03, 0.04), breaks = seq(-0.03, 0.04, 0.01), labels = percent_format(accuracy = 1)) +
    xlim(as.Date(c("1947-02-01", "2018-12-31"))) +
    labs(title = "Log-Change in U.S. Real GDP, Seasonally Adjusted Annual Rate")
```

$_{}$ \hspace*{0.75cm} https://research.stlouisfed.org/fred2/series/GDPC1



## Trend, Seasonality, Structural Change, Volatility, Outliers

```{r, echo=FALSE, fig.height=4, fig.width=11}
# Crude Oil Prices: West Texas Intermediate
x_tbl <- "DCOILWTICO" %>%
    tq_get(get = "economic.data", from = "1986-01-01", to = "2018-12-31") %>%
    mutate(price_dlog = log(price) - lag(log(price)),
           price_pch_dod = price/lag(price) - 1,
           err = price_dlog - price_pch_dod) %>%
    gather(measure, value, c(price, price_dlog, price_pch_dod, err)) 

g %+% {x_tbl %>% filter(measure == "price")} +
    # scale_y_continuous(limits = c(0, 150), breaks = seq(0, 150, 50), labels = number_format(accuracy = 1)) +
    xlim(as.Date(c("1986-01-01", "2018-12-31"))) +
    labs(title = "Crude Oil Prices: West Texas Intermediate, Dollars per Barrel, Not Seasonally Adjusted")

g %+% {x_tbl %>% filter(measure == "price_dlog")} +
    # geom_hline(yintercept = 0, color = "gray60")+
    # scale_y_continuous(limits = c(-0.425, 0.2), breaks = seq(-0.4, 0.2, 0.2), labels = percent_format(accuracy = 1)) +
    xlim(as.Date(c("1986-01-01", "2018-12-31"))) +
    labs(title = "Log-Change in Crude Oil Prices: West Texas Intermediate, Not Seasonally Adjusted")
```

$_{}$ \hspace*{0.75cm} https://research.stlouisfed.org/fred2/series/DCOILWTICO



## Trend, Seasonality, Structural Change, Volatility, Outliers

```{r, echo=FALSE, fig.height=4, fig.width=11}
# Cash Price of Corn
x_tbl <- "TFGRAIN/CORN.1" %>%
    tq_get(get = "quandl", from = "2001-01-01", to = "2018-12-31") %>%
    rename(price = cash.price) %>%
    mutate(price_dlog = log(price) - lag(log(price)),
           price_pch_dod = price/lag(price) - 1,
           err = price_dlog - price_pch_dod) %>%
    gather(measure, value, c(price, price_dlog, price_pch_dod, err)) 

g %+% {x_tbl %>% filter(measure == "price")} +
    scale_y_continuous(limits = c(1.6, 3.8), breaks = seq(2, 3.5, 0.5), labels = number_format(accuracy = 0.1)) +
    xlim(as.Date(c("2002-01-01", "2005-12-31"))) +
    labs(title = "Cash Price of Corn")

g %+% {x_tbl %>% filter(measure == "price_dlog")} +
    # geom_hline(yintercept = 0, color = "gray60")+
    # scale_y_continuous(limits = c(-0.425, 0.2), breaks = seq(-0.4, 0.2, 0.2), labels = percent_format(accuracy = 1)) +
    xlim(as.Date(c("2002-01-01", "2005-12-31"))) +
    labs(title = "Log-Change in Cash Price of Corn")
```

$_{}$ \hspace*{0.75cm} https://www.quandl.com/data/TFGRAIN/CORN




## Trend, Seasonality, Structural Change, Volatility, Outliers

- decomposition of time series into trend, seasonal and irregular component
$$
    y_t = \mu_t + \gamma_t + \varepsilon_t 
$$
where  
\hspace{1cm} $y_t$ is the observed data  
\hspace{1cm} $\mu_t$ is an slowly changing component (trend)  
\hspace{1cm} $\gamma_t$ is periodic seasonal component  
\hspace{1cm} $\varepsilon_t$ is irregular disturbance component  

- classical approach - treat trend and seasonal components as deterministic functions

- modern approach - $\mu_t$, $\gamma_t$, $\varepsilon_t$ all contain stochastic components

- we will first look at the ways how to model the irregular component, and leave seasonal and trend components for later


<!--
## Preliminaries

**Def:** A random variable $X: \Omega \rightarrow \mathbb R$ is a measurable function from the set of possible outcomes $\Omega$ to real numbers $\mathbb R$. 
\medskip

**Def:** Let $\mathbf{X}=(X_1,\ldots,X_n)$ be a vector of random variables $X_i: \Omega_i \rightarrow \mathbb R$. Let $P(\mathbf X \in A)$ be the probability that $\mathbf X \in A$ for $A \subset \mathbb R^n$. The *joint distribution function* $F_{\mathbf{X}}: \mathbb R^n \rightarrow \mathbb R$ is then 
$$
    F_{\mathbf{X}}(\mathbf{x}) = P(\mathbf X \leq x)
$$
If $\mathbf X$ is a continuous random vector and a joint probability density function $f_{\mathbf X}$ exists then
$$
    F_{\mathbf{X}}(\mathbf{x}) = \int_{-\infty}^{x_1} \ldots \int_{-\infty}^{x_n} f_{\mathbf X}(s_1,\ldots,s_n) ds_1 \ldots ds_n
$$
-->


## Preliminaries

**Def:** Stochastic process (or time series process) is a sequence of random variables. Observed time series is a particular realization of this process.

\begin{figure}[h]
    \includegraphics[trim=0cm 0cm 0cm 0cm, clip, width=9cm]{figures/rivera/fig3_4.png}
\end{figure}

<!--
Notation: $\{y_t\}$ times series, $y_t$ value at period $t$
-->



## Preliminaries

**Def:** Stochastic process $\{y_t\}$ is **strictly stationary** if joint distributions $F(y_{t_1},\ldots,y_{t_k})$ and $F(y_{{t_1}+l},\ldots, y_{{t_k}+l})$ are identical for all $l$, $k$ and all $t_1,\ldots,t_k$

\begin{figure}[h]
    \includegraphics[trim=0cm 0cm 0cm 0cm, clip, width=9cm]{figures/rivera/fig3_6A.png}
\end{figure}



## Preliminaries

**Def:** Stochastic process $\{y_t\}$ is (second order) **weakly stationary** if   
(i) $E(y_t)=\mu$ for all $t$  
(ii) $cov(y_t,y_{t-l})=\gamma_l$ for all $t$, $l$

\medskip

Note: if (i) is satisfied but (ii) the process is first order weakly stationary

Note: for $l = 0$ we get that $var(y_t) = cov(y_t,y_t) = \gamma_0$ for all $t$, which means that variance is constant over time

\begin{figure}[h]
    \includegraphics[trim=0cm 0cm 0cm 0cm, clip, width=9cm]{figures/rivera/fig3_6B.png}
\end{figure}



## Preliminaries

\begin{figure}[h]
    \includegraphics[trim=0cm 0cm 0cm 0cm, clip, width=9cm]{figures/rivera/fig3_5.png}
\end{figure}


## Preliminaries

- weak stationarity allows us to use sample moments to estimate population moments

- for example, given a weakly stationary time series $\{y_1,y_2,\ldots,y_t\}$ the first moment $E(y_t)$ can be estimated using $\frac{1}{t}\sum_{j=1}^t y_j$

- for nonstationary process $\frac{1}{t}\sum_{j=1}^t y_j$ is not a useful estimator, since $E(y_1) \neq E(y_2) \neq \ldots \neq E(y_t)$



## Preliminaries

**Def:** Stochastic process $\{\varepsilon_t\}$ is called a **white noise** if $\varepsilon_t$ are independently identically distributed with zero mean and finite variance: $E(\varepsilon_t)=0$, $Var(\varepsilon_t)=\sigma_\varepsilon^2 < \infty$, $cov(\varepsilon_t,\varepsilon_s)=0$ for all $t\neq s$.



## Box-Jenkins Methodology

Box-Jenkins methodology to modelling weakly stationary time series

1. Identification
2. Estimation
3. Checking Model Adequacy


## Box-Jenkins Methodology

1. **Indentification**
* examine **time series plots** of the data to determine if any transformations are necessary (differencing, logarithms) to get weakly stationary time series, examine series for trend (linear/nonlinear), periods of higher volatility, seasonal patterns, structural breaks, outliers, missing data, ...
* examine **autocorrelation function (ACF)** and **partial autocorrelation function (PACF)** of the transformed data to determine plausible models to be estimated
* use **Q-statistics** to test whether groups of autocorrelations are statistically significant


## Box-Jenkins Methodology

2. **Estimation**
* estimate all models considered and select the best one - coefficients should be statistically significant, **information criteria (AIC, SBC)** should be low
* model can be estimated using either **conditional likelihood method** or exact **likelihood method**


## Box-Jenkins Methodology

3. **Checking Model Adequacy**
* perform **in-sample evaluation** of the estimated model 
    * estimated coefficients should be consistent with the underlying assumption of stationarity
    * inspect residuals - if the model was well specified residuals should be very close to white-noise
        + plot residuals, look for outliers, periods in which the model does not fit the data well, evidence of structural change
        + examine ACF and PACF of the residuals to check for significant autocorrelations
        + use Q-statistics to test whether autocorrelations of residuals are statistically significant
    * check model for parameter instability and structural change
* perform **out-of-sample evaluation** of the model forecast


## Box-Jenkins Methodology

- we will now look at how the Box-Jenkins methodology works in case of a simple univariate time series model - an autoregressive model



## AR($p$) Model

- simple linear regression model with cross sectional data
$$
    y_i = \beta_0 + \beta_1 x_i + \epsilon_i
$$
- suppose we are dealing with time series rather than cross sectional data, so that
$$
    y_t = \beta_0 + \beta_1 x_t + \epsilon_t
$$
and if the explanatory variable is the lagged dependent variable $x_t=y_{t-1}$ we get
$$
    y_t = \beta_0 + \beta_1 y_{t-1} + \epsilon_t
$$
- main idea: past is prologue as it determines the present, which in turn sets the stage for future



## AR($p$) Model

- hourly time series for Akkoro Kamui's activities, before the fortress was built
$$
    \{ y_1, y_2, \ldots, y_t \} = \{ drink, drink, \ldots, drink \}
$$

- lots of time dependence here:
$$
    y_t = y_{t-1}
$$



## AR($p$) Model

- time series process $\{y_t\}$ follows autoregressive model of order 1, AR(1), if
$$
    y_t = \phi_0 + \phi_1 y_{t-1} + \varepsilon_t
$$
or equivalently, using the lag operator
$$
    (1-\phi_1 L) y_t = \phi_0 + \varepsilon_t
$$
where $\{\varepsilon_t\}$ is a white noise with $E(\varepsilon_t)=0$ and $Var(\varepsilon_t)=\sigma_\varepsilon^2$
<!-- 
At period $t-1$ given information $\mathcal I_{t-1}=\{y_1,\ldots,y_{t-1}\}$ then  

* conditional mean: $E_{t-1}(y_t)=E(y_t|\mathcal I_{t-1})=\phi_0+\phi_1 y_{t-1}$  

* conditional variance: $Vay_{t-1}(y_t)=Var(y_t|\mathcal I_{t-1})=Var(\varepsilon_t)=\sigma_\varepsilon^2$  

If $y_t$ is weakly stationary we have  

* unconditional mean: $E(y_t)=\frac{\phi_0}{1-\phi_1}$  

* unconditional variance: $Var(y_t)=\frac{\sigma_\varepsilon^2}{1-\phi_1^2}$  

The necessary and sufficient condition for an AR(1) process to be weakly stationray is $|\phi_1|<1$.
-->

- more generally, time series $\{y_t\}$ follows an autoregressive model of order $p$, AR($p$), if
$$
    y_t = \phi_0 + \phi_1 y_{t-1} + \ldots + \phi_p y_{t-p} + \varepsilon_t
$$
or equivalently, using the lag operator
$$
    (1-\phi_1 L-\ldots-\phi_p L^p) y_t = \phi_0 + \varepsilon_t
$$

<!-- 
At period $t-1$ given information $\mathcal I_{t-1}=\{y_1,\ldots,y_{t-1}\}$ we have  

* conditional mean: $E_{t-1}(y_t)=E(y_t|\mathcal I_{t-1})=\phi_0+\phi_1 y_{t-1}+ \ldots + \phi_p y_{t-p}$  

* conditional variance: $Vay_{t-1}(y_t)=Var(y_t|\mathcal I_{t-1})=Var(\varepsilon_t)=\sigma_\varepsilon^2$  

If $y_t$ is weakly stationary we have

* unconditional mean: $E(y_t)=\frac{\phi_0}{1-\phi_1-\ldots-\phi_p}$  

* unconditional variance: $Var(y_t)=\frac{\sigma_\varepsilon^2 + 2 \sum_{j=1}^{p-1} \sum_{j=i+1}^p \phi_i \phi_j \gamma_{j-i} }{1-\phi_1^2-\ldots-\phi_p^2}$, where $\gamma_l = \phi_1 \gamma_{l-1} + \ldots + \phi_p \gamma_{l-p}$ for $l>0$
-->


## AR($p$) Model

tools to determined the order $p$ of the autoregressive model given $\{y_t\}$

- Autocorrelation Function (ACF) 
- Partial Autocorrelation Function (PACF)
- Portmanteau Test - Box-Pierce test and Ljung-Box test
- Information Criteria - Akaike (AIC) and Schwarz-Bayesian (BIC)


## Autocorrelation Function (ACF)

- linear dependence between $y_t$ and $y_{t-l}$ is given by correlation coefficient $\rho_l$

- for a weakly stationary time series process$\{y_t\}$ we have 
$$
    \rho_l = \frac{cov(y_t,y_{t-l})}{\sqrt{Var(y_t)Var(y_{t-l})}} = \frac{cov(y_t,y_{t-l})}{Var(y_t)} = \frac{\gamma_l}{\gamma_0}
$$

- **theoretical autocorrelation function** is $\{\rho_1, \rho_2, \ldots\}$

- given a sample $\{y_t\}_{t=1}^T$ correlation coefficients $\rho_l$ can be estimated as 
$$
    \hat \rho_l = \frac{\sum_{t=l+1}^T (y_t - \bar y)(y_{t-l}-\bar y)}{\sum_{t=1}^T (y_t - \bar y)^2}
$$
where $\bar y = \frac{1}{T} \sum_{t=1}^T y_t$

- **sample autocorrelation function** is $\{\hat \rho_1, \hat \rho_2, \ldots\}$ 



## Autocorrelation function for AR($p$) model

- if $p=1$ then $\gamma_0=Var(y_t)=\frac{\sigma_\varepsilon^2}{1-\phi_1^2}$ and also $\gamma_l = \phi_1 \gamma_{l-1}$ for $l>0$, thus
\begin{equation}
    \rho_l = \phi_1 \rho_{l-1}
\end{equation}
and since $\rho_0=1$, we get $\rho_l = \phi_1^l$

- for weakly stationary $\{y_t\}$ it has to hold that $|\phi_1|<1$, theoretical ACF of a stationary AR(1) thus decays exponentially, in either direct or oscillating way



## Autocorrelation function for AR($p$) model

- if $p=2$ theoretical ACF for AR(2) satisfies second order difference equation
\begin{equation}
    \rho_l = \phi_1 \rho_{l-1} + \phi_2 \rho_{l-2}  \label{eq:ACF_AR2}
\end{equation}
or equivalently using the lag operator $(1-\phi_1 L - \phi_2 L^2)\rho_l = 0$

- solutions of the associated **characteristic equation**
$$
    1 -\phi_1 x - \phi_2 x^2=0
$$
are $x_{1,2} = -\frac{\phi_1 \pm \sqrt{\phi_1^2 + 4\phi_2}}{2 \phi_2}$

- their inverses $\omega_{1,2}=1/x_{1,2}$ are called the **characteristic roots** of the AR(2) model

- if $D=\phi_1^2 + 4\phi_2 >0$ then $\omega_1,\omega_2$ are real numbers, and theoretical ACF is a combination of two exponential decays

- if $D<0$ characteristic roots are complex conjugates, and theoretical ACF will resemble a dampened sine wave

- for weak stationarity all characteristic roots need to lie inside the unit circle, that is $|\omega_i|<1$ for $i=1,2$

- from equation (\ref{eq:ACF_AR2}) we get $\rho_1=\frac{\phi_1}{1-\phi_2}$ and $\rho_l = \rho_{l-1} + \phi_2 \rho_{l-2}$ for $l\geq 2$


## Autocorrelation function for AR($p$) model

- in general, theoretical ACF for AR($p$) satisfies the difference equation of order $p$
\begin{equation}
    (1-\phi_1 L-\ldots-\phi_p L^p)\rho_l = 0 
\end{equation}

- characteristic equation of the AR($p$) model is thus $1-\phi_1 x - \ldots - \phi_p x^p=0$

- AR($p$) process is weakly stationary if the characteristic roots (i.e. inverses of the solutions of the characteristic equation) lie inside of the unit circle

- plot of the theoretical ACF of a weakly stationary AR($p$) process will show a mixture of exponential decays and dampened sine waves


## Partial autocorrelation function (PACF)

- consider the following system of AR models that can be estimated by OLS
\begin{align}
    y_t &= \phi_{0,1} + \phi_{1,1} y_{t-1} + e_{1,t} \\
    y_t &= \phi_{0,2} + \phi_{1,2} y_{t-1} + \phi_{2,2} y_{t-2} + e_{2,t} \\
    y_t &= \phi_{0,3} + \phi_{1,3} y_{t-1} + \phi_{2,3} y_{t-2} + \phi_{3,3} y_{t-3} + e_{3,t} \\
        & \vdots    
\end{align}

 - estimated coefficients $\hat \phi_{1,1}, \hat \phi_{2,2}, \hat \phi_{3,3}, \ldots$ form the sample **partial autocorrelation function** (PACF)
 
 - if the time series process $\{y_t\}$ comes from an AR($p$) process, sample PACF should have $\hat \phi_{j,j}$ close to zero for $j>p$
 
- for an AR($p$) with Gaussian white noise as $T$ goes to infinity $\hat \phi_{p,p}$ converges to $\phi_p$ and $\hat \phi_{l,l}$ converges to 0 for $l>p$, in addition the asymptotic variance of $\hat \phi_{l,l}$ for $l>p$ is $1/T$

- this is the reason why the interval plotted by R in the plot of PACF is $0 \pm 2/\sqrt T$

- order of the AR process can thus be determined by finding the lag after which PACF cuts off to zero


## ACF and PACF for AR$(1)$ model

```{r, echo=FALSE}
    mymaxlag <- 20
    mynobs <- 500
```

AR(1) with $\phi_1=0.7$
```{r, echo=FALSE}
    phi <- c(0.7,0,0)
    y.sim1 <- arima.sim(model=list(order = c(length(phi),0,0), ar=phi), n=mynobs)
```

```{r, echo=FALSE, fig.height=6}
    par(mfrow=c(2,2), cex=1, mar=c(2,4,0,1))
    plot(0:mymaxlag, ARMAacf(ar=phi, lag.max=mymaxlag, pacf=FALSE), type='h', xlab="", ylab="theoretical ACF", main="")
    plot(1:mymaxlag, ARMAacf(ar=phi, lag.max=mymaxlag, pacf=TRUE), type='h', xlab="", ylab="theoretical PACF", main="")
    acf(y.sim1, type="correlation", lag=mymaxlag, ylab="sample ACF", main="")
    acf(y.sim1, type="partial", lag=mymaxlag, ylab="sample PACF", main="")
```


## ACF and PACF for AR$(1)$ model

AR(1) with $\phi_1=-0.7$
```{r, echo=FALSE}
    phi <- c(-0.7,0,0)
    y.sim1 <- arima.sim(model=list(order = c(length(phi),0,0), ar=phi), n=mynobs)
```

```{r, echo=FALSE, fig.height=6}
    par(mfrow=c(2,2), cex=1, mar=c(2,4,1,1))
    plot(0:mymaxlag, ARMAacf(ar=phi, lag.max=mymaxlag, pacf=FALSE), type='h', xlab="", ylab="theoretical ACF", main="")
    plot(1:mymaxlag, ARMAacf(ar=phi, lag.max=mymaxlag, pacf=TRUE), type='h', xlab="", ylab="theoretical PACF", main="")
    acf(y.sim1, type="correlation", lag=mymaxlag, ylab="sample ACF", main="")
    acf(y.sim1, type="partial", lag=mymaxlag, ylab="sample PACF", main="")
```


## ACF and PACF for AR$(2)$ model

AR(2) with $\phi_1=0.2$, $\phi_2=0.7$
```{r, echo=FALSE}
    phi <- c(0.2,0.7,0)
    y.sim1 <- arima.sim(model=list(order = c(length(phi),0,0), ar=phi), n=mynobs)
```

```{r, echo=FALSE, fig.height=6}
    par(mfrow=c(2,2), cex=1, mar=c(2,4,1,1))
    plot(0:mymaxlag, ARMAacf(ar=phi, lag.max=mymaxlag, pacf=FALSE), type='h', xlab="", ylab="theoretical ACF", main="")
    plot(1:mymaxlag, ARMAacf(ar=phi, lag.max=mymaxlag, pacf=TRUE), type='h', xlab="", ylab="theoretical PACF", main="")
    acf(y.sim1, type="correlation", lag=mymaxlag, ylab="sample ACF", main="")
    acf(y.sim1, type="partial", lag=mymaxlag, ylab="sample PACF", main="")
```


## ACF and PACF for AR$(2)$ model

AR(2) with $\phi_1=1.2$, $\phi_1=-0.7$
```{r, echo=FALSE}
    phi <- c(1.2,-0.7,0)
    y.sim1 <- arima.sim(model=list(order = c(length(phi),0,0), ar=phi), n=mynobs)
```

```{r, echo=FALSE, fig.height=6}
    par(mfrow=c(2,2), cex=1, mar=c(2,4,1,1))
    plot(0:mymaxlag, ARMAacf(ar=phi, lag.max=mymaxlag, pacf=FALSE), type='h', xlab="", ylab="theoretical ACF", main="")
    plot(1:mymaxlag, ARMAacf(ar=phi, lag.max=mymaxlag, pacf=TRUE), type='h', xlab="", ylab="theoretical PACF", main="")
    acf(y.sim1, type="correlation", lag=mymaxlag, ylab="sample ACF", main="")
    acf(y.sim1, type="partial", lag=mymaxlag, ylab="sample PACF", main="")
```


## ACF and PACF for AR$(p)$ model

- interactive overview of ACF and PACF for simulated AR($p$) models is  [\textcolor{red}{here}](https://janduras.shinyapps.io/ARMAsim/lec02ARMAsim.Rmd)



## Portmanteau Test

- to test $H_0: \rho_1=\ldots=\rho_m=0$ against an alternative hypothesis $H_a: \rho_j\neq 0$ for some $j \in \{1,\ldots,m\}$ following two statistics can be used:

    Box-Pierce test
    $$
        Q^*(m) = T \sum_{l=1}^m \hat \rho_l^2
    $$
    
    Ljung-Box test
    $$
        Q(m) = T(T+2) \sum_{l=1}^m \frac{\hat \rho_l^2}{T-l}
    $$

- the null hypothesis is rejected at $\alpha$\% level if the above statistics are larger than the $100(1-\alpha)$th percentile of chi-squared distribution with $m$ degrees of freedom

- note: Ljung-Box statistics tends to perform better in smaller samples

- the general recommendation is to use $m \approx \ln T$, but this depends on application

- e.g.: for monthly data with a seasonal pattern it makes sense to set $m$ to 12, 24 or 36, and for quarterly data with a seasonal pattern $m$ to 4, 8, 12



## Portmanteau Test

- these tests are also used for in-sample evaluation of model adequacy 

- if the model was correctly specified Ljung-Box $Q(m)$ statistics for the residuals of the estimated model follows chi-squared distribution with $m-g$ degrees of freedom where $g$ is the number of estimated parameters 

- for AR($p$) that includes a constant $g=p+1$



## Information Criteria

- in practice, there will be often several competing models that would be considered

- if these models are adequate and with very similar properties based on ACF, PACF, and Q statistics for residuals, information criteria can help decide which one is preferred

- main idea: information criteria combine the goodness of fit with a penalty for using more parameters



## Information Criteria

- two commonly used information criteria:

    **Akaike Information Criterion (AIC)** 
    $$
        AIC = -\frac{2}{T} \log L  + \frac{2}{T}n
    $$
    
    **Schwarz-Bayesian information criterion (BIC)**
    $$
        BIC = -\frac{2}{T} \log L  + \frac{\log T}{T}n
    $$
    in both expressions above $T$ is the sample size, $n$ is the number of parameters in the model, $L$ is the value of the likelihood function, and $\log$ is the natural logarithm

- AIC or BIC of competing models can be compared and the model that has the smallest AIC or BIC value is preferred

- BIC will always select a more parsimonious model with fewer parameters than the AIC because $\log T > 2$ and each additional parameter is thus penalized more heavily



## Information Criteria

- fundamental difference - AIC tries to select the model that most adequately approximates unknown complex data generating process with infinite number of parameters

- this true process is never in the set of candidate models that are being considered

- BIC assumes that the true model is among the set of considered candidates and tries to identify it

    - BIC performs better than AIC in large samples - it is asymptotically consistent while AIC is biased toward selecting an overparameterized model

- in small samples AIC can perform better than BIC



## Information Criteria

- some software packages report other information criteria in addition to AIC and BIC

- **Hannan-Quinn information criterion (HQ)**
$$
    HQ = -\frac{2}{T} \log L  + \frac{2 \log (\log T)}{T} n
$$
 - **corrected Akaike Information Criterion (AICc)** which is AIC with a correction for finite sample sizes to limit overfitting; for a  univariate linear model with normal residuals
$$
    AICc = AIC + \frac{2(n+1)(n+2)}{T-n-2}
$$
where $T$ is the sample size and $n$ is the number of estimated parameters

 
 
<!--
Note: for Gaussian AR(1) model
$$
    \log L 
    = \log \Big( \prod_{t=1}^T \frac{1}{\sqrt{2\pi}\hat\sigma_a} e^{-\frac{\hat a_t^2}{2\hat\sigma_a^2}} \Big)
    = - \sum_{t=1}^T \Big( \log \sqrt{2\pi} + \log \hat\sigma_a + \Big(\frac{\hat a_t}{2\hat\sigma_a^2} \Big) \Big)
    = - \frac{1}{2} \Big( T \log (2\pi) + T \log \hat\sigma_a^2 + \frac{1}{\hat\sigma_a^2} \sum_{t=1}^T \hat a_t^2 \Big)
$$
thus
$$
    AIC = 1+\log (2\pi) + \log \hat\sigma_a^2 + \frac{2}{T}n
$$
and
$$
    BIC = 1+\log (2\pi) + \log \hat\sigma_a^2 + \frac{\log T}{T}n
$$
-->



## Example: AR model for Real GNP growth rate

<!--
- an example showing the steps of estimating and checking a model for the growth rate of GNP can be found here: [lec03GNP.zip](http://myweb.ttu.edu/jduras/files/teaching/eco5316/lec03GNP.zip) \smallskip
-->

```{r}
# load magrittr package (pipe operators)
library(magrittr)
```

```{r}
# import the data on the growth rate of GDP, convert it into time series xts object
y <- scan(file = "http://faculty.chicagobooth.edu/ruey.tsay/teaching/fts3/q-gnp4791.txt") %>%
       ts(start = c(1947,2), frequency = 4) 
```

```{r}
str(y)
head(y)
tail(y)
```



## Example: AR model for Real GNP growth rate

```{r fig.height=3.5}
# load ggplot2, ggfortify and forecast packages
library(ggplot2)
library(ggfortify)
library(forecast)
# define default theme to be B&W
theme_set(theme_bw())
# plot
autoplot(y) + 
    labs(x = "", y = "", title = "Real GNP growth rate") 
```



## Example: AR model for Real GNP growth rate

```{r fig.height=2.75}
# plot ACF and PACF for y up to lag 24
ggAcf(y, lag.max = 24)
ggPacf(y, lag.max = 24)
```



## Example: AR model for Real GNP growth rate

```{r mysize=TRUE, size='\\tiny'}
# estimate an AR(1) model - there is only one significant coefficient in the PACF plot for y
m1 <- Arima(y, order = c(1,0,0))
# show the structure of object m1
str(m1)
```



## Example: AR model for Real GNP growth rate

\normalsize

```{r}
# print out results for m1
m1
```



## Example: AR model for Real GNP growth rate

```{r fig.height=6.25}
# diagnostics for AR(1) model - there seems to be a problem with remaining serial correlation at lag 2
ggtsdiag(m1, gof.lag = 16)
```



## Example: AR model for Real GNP growth rate

```{r fig.height=6.25}
# estimate an AR(2) model to deal with the problem of remaining serial correlation at lag 2
m2 <- Arima(y, order = c(2,0,0))
# diagnostics for AR(2) model shows that problem with remaining serial correlation at lag 2 is gone
ggtsdiag(m2, gof.lag = 16)
```



## Example: AR model for Real GNP growth rate

```{r fig.height=6.25}
# estimate an AR(3) model since PACF for lag 2 and 3 are comparable in size
m3 <- Arima(y, order = c(3,0,0))
# diagnostics for the AR(3) model
ggtsdiag(m3, gof.lag = 16)
```


<!--
## Example: AR model for Real GNP growth rate

```{r}
# use AIC to choose order p of the AR model
m <- ar(y, method="mle")
str(m)
# AIC prefers AR(3) to AR(2)
m$order
m$aic
```



## Example: AR model for Real GNP growth rate

```{r}
# BIC prefers AR(1) to AR(2) or AR(3)
# in general BIC puts a larger penalty on additional coefficients than AIC
BIC(m1)
BIC(m2)
BIC(m3)
```
-->


## Example: AR model for Real GNP growth rate

```{r}
# Ljung-Box test - for residuals of a model adjust the degrees of freedom m 
# by subtracting the number of parameters g
# this adjustment will not make a big difference if m is large but matters if m is small

m2.LB.lag8 <- Box.test(m2$residuals, lag = 8, type = "Ljung")
m2.LB.lag8
1-pchisq(m2.LB.lag8$statistic, df = 6)

m2.LB.lag12 <- Box.test(m2$residuals, lag = 12, type = "Ljung")
m2.LB.lag12
1-pchisq(m2.LB.lag12$statistic, df = 10)
```
